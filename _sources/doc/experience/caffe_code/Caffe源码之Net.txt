Caffe的Net
***********

概述
=======
* Blob：是基础的数据结构，用来保存学习到的参数以及网络传输过程中产生数据。
* Layer：是网络的基本单元，由此派生出了各种层。
* Net：是网络的搭建，将Layer所派生出层类组合成网络。
* Solver：是Net的求解。

Net
=====

源码文件
^^^^^^^^
* include/net.hpp
* src/net.cpp


整体概览
^^^^^^^^^

::

	template <typename Dtype>
	class Net {
	 public:
	  explicit Net(const NetParameter& param, const Net* root_net = NULL);
	  explicit Net(const string& param_file, Phase phase,
	      const Net* root_net = NULL);
	  virtual ~Net() {}

	  void Init(const NetParameter& param);

	  const vector<Blob<Dtype>*>& Forward(Dtype* loss = NULL);
	  const vector<Blob<Dtype>*>& ForwardPrefilled(Dtype* loss = NULL)
	  Dtype ForwardFromTo(int start, int end);
	  Dtype ForwardFrom(int start);
	  Dtype ForwardTo(int end);
	  /// @brief DEPRECATED; set input blobs then use Forward() instead.
	  const vector<Blob<Dtype>*>& Forward(const vector<Blob<Dtype>* > & bottom,
	      Dtype* loss = NULL);
	  void Backward();
	  void BackwardFromTo(int start, int end);
	  void BackwardFrom(int start);
	  void BackwardTo(int end);

  	  void Reshape();

	  void CopyTrainedLayersFrom(const NetParameter& param);
	  ...

	 protected:
	  void AppendTop(const NetParameter& param, const int layer_id,
	                 const int top_id, set<string>* available_blobs,
	                 map<string, int>* blob_name_to_idx);
	   /// @brief The network name
	  string name_;
	  /// @brief The phase: TRAIN or TEST
	  Phase phase_;
	  /// @brief Individual layers in the net
	  vector<shared_ptr<Layer<Dtype> > > layers_;
	  vector<string> layer_names_;
	  map<string, int> layer_names_index_;
	  vector<bool> layer_need_backward_;
	  ...
      DISABLE_COPY_AND_ASSIGN(Net);

	}

Net例子
^^^^^^^^
examples/mnist/lenet_train_test.prototxt

::

	name: "LeNet"
	layer {
	  name: "mnist"
	  type: "Data"
	  top: "data"
	  top: "label"
	  include {
	    phase: TRAIN
	  }
	  transform_param {
	    scale: 0.00390625
	  }
	  data_param {
	    source: "examples/mnist/mnist_train_lmdb"
	    batch_size: 64
	    backend: LMDB
	  }
	}
	layer {
	  name: "mnist"
	  type: "Data"
	  top: "data"
	  top: "label"
	  include {
	    phase: TEST
	  }
	  transform_param {
	    scale: 0.00390625
	  }
	  data_param {
	    source: "examples/mnist/mnist_test_lmdb"
	    batch_size: 100
	    backend: LMDB
	  }
	}
	layer {
	  name: "conv1"
	  type: "Convolution"
	  bottom: "data"
	  top: "conv1"
	  param {
	    lr_mult: 1
	  }
	  param {
	    lr_mult: 2
	  }
	  convolution_param {
	    num_output: 20
	    kernel_size: 5
	    stride: 1
	    weight_filler {
	      type: "xavier"
	    }
	    bias_filler {
	      type: "constant"
	    }
	  }
	}
	layer {
	  name: "pool1"
	  type: "Pooling"
	  bottom: "conv1"
	  top: "pool1"
	  pooling_param {
	    pool: MAX
	    kernel_size: 2
	    stride: 2
	  }
	}
	layer {
	  name: "conv2"
	  type: "Convolution"
	  bottom: "pool1"
	  top: "conv2"
	  param {
	    lr_mult: 1
	  }
	  param {
	    lr_mult: 2
	  }
	  convolution_param {
	    num_output: 50
	    kernel_size: 5
	    stride: 1
	    weight_filler {
	      type: "xavier"
	    }
	    bias_filler {
	      type: "constant"
	    }
	  }
	}
	layer {
	  name: "pool2"
	  type: "Pooling"
	  bottom: "conv2"
	  top: "pool2"
	  pooling_param {
	    pool: MAX
	    kernel_size: 2
	    stride: 2
	  }
	}
	layer {
	  name: "ip1"
	  type: "InnerProduct"
	  bottom: "pool2"
	  top: "ip1"
	  param {
	    lr_mult: 1
	  }
	  param {
	    lr_mult: 2
	  }
	  inner_product_param {
	    num_output: 500
	    weight_filler {
	      type: "xavier"
	    }
	    bias_filler {
	      type: "constant"
	    }
	  }
	}
	layer {
	  name: "relu1"
	  type: "ReLU"
	  bottom: "ip1"
	  top: "ip1"
	}
	layer {
	  name: "ip2"
	  type: "InnerProduct"
	  bottom: "ip1"
	  top: "ip2"
	  param {
	    lr_mult: 1
	  }
	  param {
	    lr_mult: 2
	  }
	  inner_product_param {
	    num_output: 10
	    weight_filler {
	      type: "xavier"
	    }
	    bias_filler {
	      type: "constant"
	    }
	  }
	}
	layer {
	  name: "accuracy"
	  type: "Accuracy"
	  bottom: "ip2"
	  bottom: "label"
	  top: "accuracy"
	  include {
	    phase: TEST
	  }
	}
	layer {
	  name: "loss"
	  type: "SoftmaxWithLoss"
	  bottom: "ip2"
	  bottom: "label"
	  top: "loss"
	}


成员变量
^^^^^^^^^
成员变量较多，选主要的

::

	  /// @brief The network name
	  string name_; //网络的名字
	  /// @brief The phase: TRAIN or TEST
	  Phase phase_; // 表明网络用于训练还是测试
	  /// @brief Individual layers in the net
	  vector<shared_ptr<Layer<Dtype> > > layers_; // 多个Layer组成
	  vector<string> layer_names_; // 多个Layer的名字
	  map<string, int> layer_names_index_; // 多个Layer名字和索引的映射
	  vector<bool> layer_need_backward_; // 多个Layer是否需要后向传播
	  /// @brief the blobs storing intermediate results between the layer.
	  vector<shared_ptr<Blob<Dtype> > > blobs_; // 多个Blobb存储的是中间结果
	  vector<string> blob_names_; // 多个Blob的名字
	  map<string, int> blob_names_index_; // 多个Blob名字和索引的映射
	  vector<bool> blob_need_backward_; // Blob是否保存后向传播，这一层的top就是下一层的bottom,网络是一层一层堆起来的。
	  /// bottom_vecs stores the vectors containing the input for each layer.
	  /// They don't actually host the blobs (blobs_ does), so we simply store
	  /// pointers.
	  vector<vector<Blob<Dtype>*> > bottom_vecs_; // 整个网络所有网络层的bottom blob指针
	  vector<vector<int> > bottom_id_vecs_; //存储整个网络所有网络层的bottom blob的ID  
	  vector<vector<bool> > bottom_need_backward_; //整个网络所有网络层的bottom blob是否需要backward。top blob不需要backward
	  /// top_vecs stores the vectors containing the output for each layer
	  vector<vector<Blob<Dtype>*> > top_vecs_; // 整个网络所有网络层的top blob指针，bottom blob和top blob会有重叠。
	  vector<vector<int> > top_id_vecs_; //存储整个网络所有网络层的top blob的ID  
	  /// Vector of weight in the loss (or objective) function of each net blob, 
	  /// indexed by blob_id.
	  vector<Dtype> blob_loss_weights_;
	  vector<vector<int> > param_id_vecs_;
	  vector<int> param_owners_;
	  vector<string> param_display_names_;
	  vector<pair<int, int> > param_layer_indices_;
	  map<string, int> param_names_index_;
	  /// blob indices for the input and the output of the net
	  vector<int> net_input_blob_indices_;
	  vector<int> net_output_blob_indices_;
	  vector<Blob<Dtype>*> net_input_blobs_;
	  vector<Blob<Dtype>*> net_output_blobs_;
	  /// The parameters in the network.
	  vector<shared_ptr<Blob<Dtype> > > params_;
	  vector<Blob<Dtype>*> learnable_params_;
	  /**
	   * The mapping from params_ -> learnable_params_: we have
	   * learnable_param_ids_.size() == params_.size(),
	   * and learnable_params_[learnable_param_ids_[i]] == params_[i].get()
	   * if and only if params_[i] is an "owner"; otherwise, params_[i] is a sharer
	   * and learnable_params_[learnable_param_ids_[i]] gives its owner.
	   */
	  vector<int> learnable_param_ids_;
	  /// the learning rate multipliers for learnable_params_
	  vector<float> params_lr_;
	  vector<bool> has_params_lr_;
	  /// the weight decay multipliers for learnable_params_
	  vector<float> params_weight_decay_;
	  vector<bool> has_params_decay_;
	  /// The bytes of memory used by this net
	  size_t memory_used_;
	  /// Whether to compute and display debug info for the net.
	  bool debug_info_;
	  /// The root net that actually holds the shared layers in data parallelism
	  const Net* const root_net_;
* Net由于是由多个Layer组成，内部的变量较多。


成员函数
^^^^^^^^^


待续。。。。。。。。。

参考
=====
* https://www.zhihu.com/question/27982282/answer/39350629
* http://blog.csdn.net/iamzhangzhuping/article/details/50537240